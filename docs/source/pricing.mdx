# Hugging Face Inference Endpoint Pricing

<div class="flex md:justify-start mb-2 text-gray-400 items-center">
  <a href="https://ui.endpoints.huggingface.co/new">
    <button class="shadow-sm bg-white bg-gradient-to-br from-gray-100/20 to-gray-200/60 hover:to-gray-100/70 text-gray-700 py-1.5 rounded-lg ring-1 ring-gray-300/60 hover:ring-gray-300/30 font-semibold active:shadow-inner px-5">
      Deploy a model
    </button>
  </a>
  <span class="mx-4 ">Or</span>
  <a
    href="mailto:api-enterprise@huggingface.co"
    class="underline"
  >
    Request a quote
  </a>
</div>

With Inference Endpoints, you can easily deploy machine learning models on dedicated infrastructure. When you create an Endpoint, you can select the instance type to deploy and scale your model, which defines an hourly price. Inference Endpoints is accessible to Hugging Face accounts with an active subscription and a credit card on file. At the end of the subscription period, the user or organization account will be charged for the compute resources used while Endpoints are up and running, that is in the “running” state.

You can find below the hourly pricing for all available instances for Inference Endpoints, as well as examples showing how the pricing is applied.

## Instance pricing

The pricing below shows public on-demand pricing for Inference Endpoints. You or your organization will only be billed while your Endpoint is running. While the prices are by the hour, the actual cost will be calculated by the second.

### CPU Instances

The table below shows currently available CPU instances and their hourly pricing. If the instance type cannot be selected in the application, you need to request a quota to use it. [Request Instance Quota](mailto:api-enterprise@huggingface.co?subject=Quota%20increase%20HF%20Endpoints&body=Hello,%0D%0A%0D%0AI%20would%20like%20to%20request%20access/quota%20increase%20for%20{INSTANCE%20TYPE}%20for%20the%20following%20account%20{HF%20ACCOUNT}.).

| Provider | Instance Size | hourly rate | vCPUs | Memory | Architecture          |
| -------- | ------------- | ----------- | ----- | ------ | --------------------- |
| aws      | small         | $0.06       | 1     | 2GB    | Intel Xeon - Ice Lake |
| aws      | medium        | $0.12       | 2     | 4GB    | Intel Xeon - Ice Lake |
| aws      | large         | $0.24       | 4     | 8GB    | Intel Xeon - Ice Lake |
| aws      | xlarge        | $0.48       | 8     | 16GB   | Intel Xeon - Ice Lake |
| azure    | small         | $0.06       | 1     | 2GB    | Intel Xeon            |
| azure    | medium        | $0.12       | 2     | 4GB    | Intel Xeon            |
| azure    | large         | $0.24       | 4     | 8GB    | Intel Xeon            |
| azure    | xlarge        | $0.48       | 8     | 16GB   | Intel Xeon            |

### GPU Instances

The table below shows currently available GPU instances and their pricing. If the instance type cannot be selected in the application, you need to request a quota to use it. [Request Instance Quota](mailto:api-enterprise@huggingface.co?subject=Quota%20increase%20HF%20Endpoints&body=Hello,%0D%0A%0D%0AI%20would%20like%20to%20request%20access/quota%20increase%20for%20{INSTANCE%20TYPE}%20for%20the%20following%20account%20{HF%20ACCOUNT}.).

| Provider | Instance Size | hourly rate | GPUs | Memory | Architecture |
| -------- | ------------- | ----------- | ---- | ------ | ------------ |
| aws      | small         | $0.60       | 1    | 14GB   | NVIDIA T4    |
| aws      | medium        | $1.30       | 1    | 24GB   | NVIDIA A10G  |
| aws      | large         | $4.50       | 4    | 56GB   | NVIDIA T4    |
| aws      | xlarge        | $6.50       | 1    | 80GB   | NVIDIA A100  |
| aws      | xxlarge       | $7.00       | 4    | 96GB   | NVIDIA A10G  |
| aws      | xxxlarge      | $45.00      | 8    | 640GB  | NVIDIA A100  |


## Pricing Examples

Below are pricing examples for a simple Endpoint and more advanced Endpoint configuration.

### Example 1: Simple Endpoint

**Endpoint configuration:**

- Cloud: AWS
- Region: us-east-1
- Instance type: CPU - medium (0.12$/h)
- Autoscaling:
  - Min replica: 1
  - Max replica: 1

**Cost calculation:**

`Instance hourly rate * (hours * # min replica)`

**Hourly cost:**

`$0.12/h * (1h * 1) = $0.12/h`

**Monthly cost:**

`$0.12/h * (730h * 1) = $87.6/month`

### Example 2: Advanced Endpoint with Autoscaling

**Endpoint configuration:**

- Cloud: AWS
- Region: us-east-1
- Instance type: GPU - small ($0.6/h)
- Autoscaling:
  - Min replica: 1
  - Max replica: 3
  - Every hour, a spike of load makes the Endpoint automatically scale from 1 to 3 replicas during 15 minutes

**Cost calculation:**

`Instance hourly rate * ((non scale-out hours * # min replica) + (scale-out hours * # additional replicas))`

**Hourly cost:**

`$0.6/h * ((0.75h * 1) + (0.25h * 2)) = $0.75/hour`

**Monthly cost:**

`$0.6/h * ((547.5h * 1) + (182.5h * 2)) = 547.5$/month`
