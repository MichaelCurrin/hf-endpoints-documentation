# Hugging Face Inference Endpoints


ðŸ¤— Inference Endpoints offers a secure production solution to easily deploy Machine Learning models on dedicated and autoscaling infrastructure managed by Hugging Face. Any transformers and sentence-transformers model hosted on the Hub should work out of the box.

A Hugging Face Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models) and has an account-wide globally unique name. During the creation of an Endpoint, the service will create uniquely encapsulated image artifacts that are either built from the model you select, or based on a custom-provided container image. Those image artifacts are completely decoupled from the Hugging Face Hub source repositories, to achieve the highest security and reliability standards.

Hugging Face Endpoints support all of the [Transformers and Sentence-Transformers tasks](/supported_tasks) and can support [custom tasks such as custom pre- & post-processing](/guides/custom_handler).

In addition, Endpoints gives you the option to use a custom container image you manage on an external service, for instance, hosted on [Docker hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). 

![creation-flow](../assets/creation-flow.png)

## Documentation and Examples

TODO: add links of the documentation